{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_len=12 \n",
    "sensor_num=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adjmatrix = np.load('train_adjmatrix.npy')\n",
    "test_adjmatrix = np.load('test_adjmatrix.npy')\n",
    "\n",
    "train_inputdata = np.load('train_inputdata.npy')\n",
    "test_inputdata = np.load('test_inputdata.npy')\n",
    "\n",
    "train_pollutiondata = np.load('train_pollutiondata.npy')\n",
    "test_pollutiondata = np.load('test_pollutiondata.npy')\n",
    "\n",
    "print(train_adjmatrix.shape)\n",
    "print(test_adjmatrix.shape)\n",
    "\n",
    "print(train_inputdata.shape)\n",
    "print(test_inputdata.shape)\n",
    "\n",
    "print(train_pollutiondata.shape)\n",
    "print(test_pollutiondata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train_adjmatrix.shape[0]\n",
    "test_num = test_adjmatrix.shape[0]\n",
    "\n",
    "train_dataset= np.concatenate((train_adjmatrix.reshape(train_num,-1),train_inputdata.reshape(train_num,-1), train_pollutiondata),1)\n",
    "test_dataset=  np.concatenate((test_adjmatrix.reshape(test_num,-1),test_inputdata.reshape(test_num,-1), test_pollutiondata),1)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconder Topology Model\n",
    "# DNN_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class fc_cnn_model(nn.Module):\n",
    "    def __init__(self, nnode, nfeat, enconder_out_dim):\n",
    "        super(fc_cnn_model, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.cnn2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.cnn3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=(1, 1))\n",
    "        \n",
    "        self.fc_encoder = nn.Linear(nnode*nnode,enconder_out_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(nnode*nfeat+enconder_out_dim, nnode)\n",
    "        \n",
    "        \n",
    "        self.nnode = nnode\n",
    "        self.nfeat = nfeat\n",
    "        self.enconder_out_dim = enconder_out_dim\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        x = x.reshape(-1, 1, self.nnode, self.nfeat)\n",
    "\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = F.relu(self.cnn3(x))\n",
    "\n",
    "        x = x.reshape(-1, self.nnode* self.nfeat)\n",
    "        \n",
    "        adj = adj.reshape(-1, self.nnode**2)\n",
    "        adj_coder = F.relu(self.fc_encoder(adj))\n",
    "\n",
    "        x =torch.cat((x, adj_coder),1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = x.reshape(-1,self.nnode)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "\n",
    "model = fc_cnn_model(nnode=100,\n",
    "            nfeat=keep_len,\n",
    "            enconder_out_dim=100)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "lr_milestones = [30, 50]\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "    \n",
    "if not os.path.isdir('results'):\n",
    "    os.mkdir('results')\n",
    "    \n",
    "best_loss = float('inf')\n",
    "# Train model\n",
    "for epoch in range(60):\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    total_loss_train = AverageMeter()\n",
    "    for index, data in enumerate(trainloader):\n",
    "\n",
    "        model.train()\n",
    "        batch_size =len(data)\n",
    "\n",
    "        train_adj = data[:,:10000].float()\n",
    "        train_input_x = data[:,10000:-1].float()\n",
    "        train_Y = data[:,-1].long()\n",
    "\n",
    "        train_adj = train_adj.reshape(batch_size, 100, 100)\n",
    "        \n",
    "        train_input_x = train_input_x.reshape(batch_size, 100, keep_len)\n",
    "\n",
    "        train_adj, train_input_x, train_Y  = train_adj.cuda(), train_input_x.cuda(), train_Y.cuda()\n",
    "\n",
    "        train_Y_hat = model(train_input_x, train_adj)\n",
    "        \n",
    "\n",
    "        batch_loss = criterion(train_Y_hat, train_Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train.update(batch_loss.item(), batch_size)\n",
    "        if index%10==0:\n",
    "            print('Epoch: {}, index: {}, train loss: {:.4f}'.format(epoch+1,int(index), total_loss_train.avg))\n",
    "            file = open(\"./results/loss2.txt\",\"a\")\n",
    "            file.write(\"Epoch = {}, index = {}  \".format(epoch+1,int(index)))\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"train_loss = {:.4f}  \".format(total_loss_train.avg))\n",
    "            file.write(\"\\n\")\n",
    "            file.close\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_test = AverageMeter()\n",
    "    right_num_top1 = 0\n",
    "    right_num_top5 = 0\n",
    "    right_num_top10 = 0\n",
    "    for index_test, test_data in enumerate(testloader):\n",
    "        \n",
    "        batch_size =len(test_data)\n",
    "\n",
    "        test_adj = test_data[:,:10000].float()\n",
    "        test_input_x = test_data[:,10000:-1].float()\n",
    "        test_Y = test_data[:,-1].long()\n",
    "\n",
    "        test_adj = test_adj.reshape(batch_size, 100, 100)\n",
    "        \n",
    "        test_input_x = test_input_x.reshape(batch_size, 100, keep_len)\n",
    "\n",
    "        test_adj, test_input_x, test_Y  = test_adj.cuda(), test_input_x.cuda(), test_Y.cuda()\n",
    "\n",
    "        test_Y_hat = model(test_input_x, test_adj)\n",
    "\n",
    "\n",
    "        loss_test = criterion(test_Y_hat, test_Y)\n",
    "\n",
    "        total_loss_test.update(loss_test.item(), test_input_x.size(0))\n",
    "        \n",
    "        gt_node =  test_Y.detach().cpu().numpy()\n",
    "        \n",
    "        test_Y_hat = F.log_softmax(test_Y_hat).detach().cpu().numpy()\n",
    "        gnn_pre_node = np.argmax(test_Y_hat, axis=1)\n",
    "        \n",
    "        right_num_top1+=np.sum(gt_node==gnn_pre_node)\n",
    "        for j in range(batch_size):\n",
    "            gt_node_sample = gt_node[j]\n",
    "            gnn_pre_node_top5 = np.argpartition(test_Y_hat[j,:], -5)[-5:]\n",
    "            gnn_pre_node_top10 = np.argpartition(test_Y_hat[j,:], -10)[-10:]\n",
    "            if np.isin(gt_node_sample, gnn_pre_node_top5):\n",
    "                right_num_top5+=1\n",
    "            if np.isin(gt_node_sample, gnn_pre_node_top10):\n",
    "                right_num_top10+=1\n",
    "                \n",
    "    print('*'*50)\n",
    "    print('Epoch = ', epoch+1)        \n",
    "    print('test loss: {:.4f}, test accuracy top1:{:.5f}, test accuracy top5:{:.5f}, test accuracy top10:{:.5f}'.format(total_loss_test.avg, right_num_top1/len(test_dataset), right_num_top5/len(test_dataset), right_num_top10/len(test_dataset)))\n",
    "    \n",
    "    file = open(\"./results/results2.txt\",\"a\")\n",
    "    file.write(\"test_loss = {:.4f}  \".format(total_loss_test.avg))\n",
    "    file.write(\"test_accuracy_top1 = {:.5f}  \".format(right_num_top1/len(test_dataset)))\n",
    "    file.write(\"test_accuracy_top5 = {:.5f}  \".format(right_num_top5/len(test_dataset)))\n",
    "    file.write(\"test_accuracy_top10 = {:.5f}  \".format(right_num_top10/len(test_dataset)))\n",
    "    file.write(\"\\n\")\n",
    "    file.close\n",
    "    \n",
    "    if total_loss_test.avg<best_loss:\n",
    "        best_loss = total_loss_test.avg\n",
    "        model_state = {\n",
    "            'net_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch':epoch,\n",
    "            'best_loss':best_loss\n",
    "        }\n",
    "       \n",
    "        save_point = './checkpoint/2_DNN_CNN'\n",
    "        torch.save(model_state, save_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
