{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = str('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_len=12 \n",
    "sensor_num=6\n",
    "direction = 2\n",
    "order_num=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_adjmatrix = np.load('./data/train_adjmatrix.npy')\n",
    "test_adjmatrix = np.load('./data/test_adjmatrix.npy')\n",
    "\n",
    "train_inputdata = np.load('./data/train_inputdata.npy')\n",
    "test_inputdata = np.load('./data/test_inputdata.npy')\n",
    "\n",
    "train_pollutiondata = np.load('./data/train_pollutiondata.npy')\n",
    "test_pollutiondata = np.load('./data/test_pollutiondata.npy')\n",
    "\n",
    "\n",
    "print(train_adjmatrix.shape)\n",
    "print(test_adjmatrix.shape)\n",
    "\n",
    "print(train_inputdata.shape)\n",
    "print(test_inputdata.shape)\n",
    "\n",
    "print(train_pollutiondata.shape)\n",
    "print(test_pollutiondata.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_random_walk_matrix(adj_mx):\n",
    "    adj_mx = sp.coo_matrix(adj_mx)\n",
    "    d = np.array(adj_mx.sum(1))\n",
    "    d_inv = np.power(d, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    random_walk_mx = d_mat_inv.dot(adj_mx).todense()\n",
    "    return random_walk_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_supports =[]\n",
    "for i in range(len(train_adjmatrix)):\n",
    "    adj_sample = train_adjmatrix[i].astype(float)\n",
    "    adj_sample = adj_sample-np.eye(100)\n",
    "    supports_sample = []\n",
    "    supports_sample.append(calculate_random_walk_matrix(adj_sample).T)\n",
    "    if direction==2:\n",
    "        supports_sample.append(calculate_random_walk_matrix(adj_sample.T).T)\n",
    "    train_supports.append(supports_sample)\n",
    "    \n",
    "train_supports = np.array(train_supports)\n",
    "print(train_supports.shape)\n",
    "\n",
    "test_supports =[]\n",
    "for i in range(len(test_adjmatrix)):\n",
    "    adj_sample = test_adjmatrix[i].astype(float)\n",
    "    adj_sample = adj_sample-np.eye(100)\n",
    "    supports_sample = []\n",
    "    supports_sample.append(calculate_random_walk_matrix(adj_sample).T)\n",
    "    if direction==2:\n",
    "        supports_sample.append(calculate_random_walk_matrix(adj_sample.T).T)\n",
    "    test_supports.append(supports_sample)\n",
    "    \n",
    "test_supports = np.array(test_supports)\n",
    "print(test_supports.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train_inputdata.shape[0]\n",
    "test_num = test_inputdata.shape[0]\n",
    "\n",
    "\n",
    "train_dataset= np.concatenate((train_supports.reshape(train_num,-1),train_inputdata.reshape(train_num,-1), train_pollutiondata),1)\n",
    "test_dataset=  np.concatenate((test_supports.reshape(test_num,-1),test_inputdata.reshape(test_num,-1), test_pollutiondata),1)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class DiffusionGCN(nn.Module):\n",
    "    def __init__(self, node_num, dim_in, dim_out, order, supports_len=2):\n",
    "        #order must be integer\n",
    "        super(DiffusionGCN, self).__init__()\n",
    "        self.node_num = node_num\n",
    "        \n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.order = order\n",
    "        self.supports_len = supports_len\n",
    "        \n",
    "        \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(size=(dim_in*(order*self.supports_len+1), dim_out)))\n",
    "        self.biases = nn.Parameter(torch.FloatTensor(size=(dim_out,)))\n",
    "        \n",
    "        nn.init.xavier_normal_(self.weight.data, gain=1.414)\n",
    "        nn.init.constant_(self.biases.data, val=0.)\n",
    "        \n",
    "\n",
    "    def forward(self, x, supports):\n",
    "        #shape of x is [B, N, D]\n",
    "        #shape of supports is [supports_len, B, N, D]\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        assert x.shape[1] == self.node_num and self.dim_in == x.shape[2]\n",
    "        \n",
    "        out = [x]\n",
    "        x0 = x\n",
    "        for support in supports:\n",
    "            x1 = torch.einsum('bij, bjk -> bik', support, x0)\n",
    "            out.append(x1)\n",
    "            for k in range(2, self.order+1):\n",
    "                x2 = 2 * torch.einsum('bij, bjk -> bik', support, x1) - x0\n",
    "                out.append(x2)\n",
    "                x1, x0 = x2, x1\n",
    "        out = torch.cat(out,dim=-1)     #B, N, D, order\n",
    "        out = out.reshape(batch_size*self.node_num, -1)     #B*N, D\n",
    "        out = torch.matmul(out, self.weight)  # (batch_size * self._num_nodes, output_size)\n",
    "        out = torch.add(out, self.biases)\n",
    "        out = out.reshape(batch_size, self.node_num, self.dim_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nnode, nfeat, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.cnn2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.cnn3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=(1, 1))\n",
    "        \n",
    "        self.gc1 = DiffusionGCN(node_num=nnode, dim_in=12, dim_out=24, order=order_num, supports_len=2)\n",
    "        self.gc2 = DiffusionGCN(node_num=nnode, dim_in=24, dim_out=1, order=order_num, supports_len=2)\n",
    "        self.fc = nn.Linear(100, nnode)\n",
    "        \n",
    "        self.nnode = nnode\n",
    "        self.nfeat = nfeat\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, supports):\n",
    "        \n",
    "        x = x.reshape(-1, 1, self.nnode, self.nfeat)\n",
    "\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = F.relu(self.cnn3(x))\n",
    "        x = x.reshape(-1, self.nnode, self.nfeat)\n",
    "\n",
    "        x = F.relu(self.gc1(x, supports))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.gc2(x, supports))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        x = x.reshape(-1, self.nnode)\n",
    "        x = self.fc(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "\n",
    "model = GCN(nnode=100,\n",
    "            nfeat=keep_len,\n",
    "            dropout=0.1\n",
    "           )\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "lr_milestones = [30, 50]\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "    \n",
    "if not os.path.isdir('results'):\n",
    "    os.mkdir('results')\n",
    "        \n",
    "best_loss = float('inf')\n",
    "# Train model\n",
    "for epoch in range(60):\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    total_loss_train = AverageMeter()\n",
    "    for index, data in enumerate(trainloader):\n",
    "\n",
    "        model.train()\n",
    "        batch_size =len(data)\n",
    "\n",
    "        train_supports = data[:,:direction*10000].float()\n",
    "        train_input_x = data[:,direction*10000:-1].float()\n",
    "        train_Y = data[:,-1].long()\n",
    "\n",
    "        train_supports = train_supports.reshape(batch_size, direction, 100, 100)\n",
    "        train_supports = torch.transpose(train_supports,0,1)\n",
    "        \n",
    "        train_input_x = train_input_x.reshape(batch_size, 100, keep_len)\n",
    "\n",
    "        train_supports, train_input_x, train_Y  = train_supports.cuda(), train_input_x.cuda(), train_Y.cuda()\n",
    "\n",
    "        train_Y_hat = model(train_input_x, train_supports)\n",
    "\n",
    "        batch_loss = criterion(train_Y_hat, train_Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train.update(batch_loss.item(), batch_size)\n",
    "        if index%10==0:\n",
    "            print('Epoch: {}, index: {}, train loss: {:.4f}'.format(epoch+1,int(index), total_loss_train.avg))\n",
    "            file = open(\"./results/loss.txt\",\"a\")\n",
    "            file.write(\"Epoch = {}, index = {}  \".format(epoch+1,int(index)))\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"train_loss = {:.4f}  \".format(total_loss_train.avg))\n",
    "            file.write(\"\\n\")\n",
    "            file.close\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_test = AverageMeter()\n",
    "    right_num_top1 = 0\n",
    "    right_num_top5 = 0\n",
    "    right_num_top10 = 0\n",
    "    for index_test, test_data in enumerate(testloader):\n",
    "        \n",
    "        batch_size =len(test_data)\n",
    "\n",
    "        test_supports = test_data[:,:direction*10000].float()\n",
    "        test_input_x = test_data[:,direction*10000:-1].float()\n",
    "        test_Y = test_data[:,-1].long()\n",
    "\n",
    "        test_supports = test_supports.reshape(batch_size, direction, 100, 100)\n",
    "        test_supports = torch.transpose(test_supports,0,1)\n",
    "        \n",
    "        test_input_x = test_input_x.reshape(batch_size, 100, keep_len)\n",
    "\n",
    "        test_supports, test_input_x, test_Y  = test_supports.cuda(), test_input_x.cuda(), test_Y.cuda()\n",
    "\n",
    "        test_Y_hat = model(test_input_x, test_supports)\n",
    "\n",
    "\n",
    "        loss_test = criterion(test_Y_hat, test_Y)\n",
    "\n",
    "        total_loss_test.update(loss_test.item(), test_input_x.size(0))\n",
    "        \n",
    "        gt_node =  test_Y.detach().cpu().numpy()\n",
    "        \n",
    "        test_Y_hat = F.log_softmax(test_Y_hat).detach().cpu().numpy()\n",
    "        gnn_pre_node = np.argmax(test_Y_hat, axis=1)\n",
    "        \n",
    "        right_num_top1+=np.sum(gt_node==gnn_pre_node)\n",
    "        for j in range(batch_size):\n",
    "            gt_node_sample = gt_node[j]\n",
    "            gnn_pre_node_top5 = np.argpartition(test_Y_hat[j,:], -5)[-5:]\n",
    "            gnn_pre_node_top10 = np.argpartition(test_Y_hat[j,:], -10)[-10:]\n",
    "            if np.isin(gt_node_sample, gnn_pre_node_top5):\n",
    "                right_num_top5+=1\n",
    "            if np.isin(gt_node_sample, gnn_pre_node_top10):\n",
    "                right_num_top10+=1\n",
    "                \n",
    "    print('*'*50)\n",
    "    print('Epoch = ', epoch+1)             \n",
    "    print('test loss: {:.4f}, test accuracy top1:{:.5f}, test accuracy top5:{:.5f}, test accuracy top10:{:.5f}'.format(total_loss_test.avg, right_num_top1/len(test_dataset), right_num_top5/len(test_dataset), right_num_top10/len(test_dataset)))\n",
    "    \n",
    "    file = open(\"./results/results.txt\",\"a\")\n",
    "    file.write(\"test_loss = {:.4f}  \".format(total_loss_test.avg))\n",
    "    file.write(\"test_accuracy_top1 = {:.5f}  \".format(right_num_top1/len(test_dataset)))\n",
    "    file.write(\"test_accuracy_top5 = {:.5f}  \".format(right_num_top5/len(test_dataset)))\n",
    "    file.write(\"test_accuracy_top10 = {:.5f}  \".format(right_num_top10/len(test_dataset)))\n",
    "    file.write(\"\\n\")\n",
    "    file.close\n",
    "    \n",
    "    if total_loss_test.avg<best_loss:\n",
    "        best_loss = total_loss_test.avg\n",
    "        model_state = {\n",
    "            'net_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch':epoch,\n",
    "            'best_loss':best_loss\n",
    "        }\n",
    "       \n",
    "        save_point = './checkpoint/CNN_DCN'\n",
    "        torch.save(model_state, save_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
